---
title: PythonでIntel NCStick2
date: 2019-10-12
tags: ["DeepLearning", "NCStick2", "openVINO", "RaspberryPi", "python"]
excerpt: PythonでIntel NCStick2を操作する
---

# 事前準備
何
はなくともPythonが必要。
ここではPythonは3.7系を使う。(3.5ではエラーになった。なんで??)  
実際には3.7.4を使用して動作確認。  

Pythonはpyenvでインストールしておくのが便利。インストール方法は 
[pyenvのインストール]({{ site.baseurl }}/2019/06/27/pyenv.html) を参照。  


## モジュールのインストール

必要なモジュール類をインストールする。  

```bash
sudo apt install libatlas-base-dev
sudo apt install libgfortran-6-dev
pip install numpy
pip install opencv-python
```


# 参考  

~~パクった~~ 参考にしたのは以下のあたり  
解説や実行方法などはこっちを参照してちょ(相変わらずの他力本願ぶり...))

- [JELLYWARE ゼロから学ぶディープラーニング推論](http://jellyware.jp/openvino/)  
    - [Inference Engineを学んで感情分類](http://jellyware.jp/kurage/openvino/c07_ie_emotion.html)  
    - [リアルタイム顔検出](http://jellyware.jp/kurage/openvino/c08_face_detection.html)  
    - [リアルタイム感情分析アプリ](http://jellyware.jp/kurage/openvino/c09_emotion_app.html)  

顔検出＋感情分類の参照元はカメラキャプチャした画像を処理しているけど、手元に適当なカメラがなかったので、JPEGファイルで実行するようにしてある(こっちの方が余計な処理なくて分かりやすいかな、と思ったのもある)。


# 感情分類

感情分類のプログラムを試す。  

### 準備

モデルデータをダウンロードしておく。  

```bash
mkdir FP16
wget https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.bin -P FP16/
wget https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.xml -P FP16/
```

使用する顔画像(顔部分のみ切り出し)をface.jpgとして保存しておく。  


### プログラム

試したソースはこちら。  

```python
# 感情分析

# モジュール読み込み 
import sys
import cv2
import numpy as np
sys.path.append('/opt/intel/openvino/python/python3.7')
from openvino.inference_engine import IENetwork, IEPlugin

# =======================================================
# 使用する画像ファイル名
image_filename = 'face.jpg'

# パラメータが指定されていれば画像ファイルとして使用
args = sys.argv
if len(args) > 1 :
    image_filename = args[1]

# ターゲットデバイスの指定 
plugin = IEPlugin(device="MYRIAD")

# モデルの読み込み（感情分類） 
net_emotion = IENetwork(model='FP16/emotions-recognition-retail-0003.xml', weights='FP16/emotions-recognition-retail-0003.bin')
exec_net_emotion = plugin.load(network=net_emotion)

# 各感情の文字列をリスト化 
list_emotion    = ['neutral', 'happy', 'sad',    'surprise', 'anger']
list_emotion_jp = ['無表情',  '幸福',  '悲しみ', '驚き',     '怒り']

# 入力画像読み込み 
frame = cv2.imread(image_filename)

# 入力データフォーマットへ変換 
img = cv2.resize(frame, (64, 64)) # サイズ変更 
img = img.transpose((2, 0, 1))    # HWC > CHW 
img = np.expand_dims(img, axis=0) # 次元合せ 

# 推論実行 
out = exec_net_emotion.infer(inputs={'data': img})

# 結果取り出し
out = out['prob_emotion']
out = np.squeeze(out) #不要な次元の削減 

# 結果の最大値を持つインデックスを取得
index_max = np.argmax(out)

# 結果表示
print('\n---------------------\n結果')
print(f'{index_max}   {list_emotion[index_max]}({list_emotion_jp[index_max]}): {out[index_max]}')
# 結果の詳細表示
print('\n---------------------\n詳細')
print(f'スコア：{out}')
for i in range(out.size) :
    print(f'{i}   {list_emotion[i]}({list_emotion_jp[i]}): {out[i]}')


```





# 顔検出

顔検出のプログラムを試す。  
結果画像を表示するので、X環境での実行必須。  


### 準備

モデルデータをダウンロードしておく。  

```bash
mkdir FP16
wget https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/face-detection-retail-0004/FP16/face-detection-retail-0004.bin -P FP16/
wget https://download.01.org/opencv/2019/open_model_zoo/R1/models_bin/face-detection-retail-0004/FP16/face-detection-retail-0004.xml -P FP16/
```

使用する画像をphoto.jpgとして保存しておく。  
または実行時にコマンドラインパラメータとしてファイル名を指定。  

### プログラム

試したソースはこちら。  

```python
# 顔検出

# モジュール読み込み 
import sys
import cv2
import numpy as np
sys.path.append('/opt/intel/openvino/python/python3.7')
from openvino.inference_engine import IENetwork, IEPlugin

# =======================================================
# アスペクト比固定でリサイズする関数
def scale_to_width(img, width):
    scale = width / img.shape[1]
    return cv2.resize(img, dsize=None, fx=scale, fy=scale)

# =======================================================
# 使用する画像ファイル名
image_filename = 'photo.jpg'

# パラメータが指定されていれば画像ファイルとして使用
args = sys.argv
if len(args) > 1 :
    image_filename = args[1]

# ターゲットデバイスの指定 
plugin = IEPlugin(device="MYRIAD")

# モデルの読み込み（顔検出） 
net_detect = IENetwork(model='FP16/face-detection-retail-0004.xml', weights='FP16/face-detection-retail-0004.bin')
exec_net_detect = plugin.load(network=net_detect)

# 入力画像読み込み 
frame = cv2.imread(image_filename)

# 入力データフォーマットへ変換 
img = cv2.resize(frame, (300, 300)) # サイズ変更 
img = img.transpose((2, 0, 1))      # HWC > CHW 
img = np.expand_dims(img, axis=0)   # 次元合せ 

# 推論実行 
out = exec_net_detect.infer(inputs={'data': img})

# 出力から必要なデータのみ取り出し 
out = out['detection_out']
out = np.squeeze(out) #サイズ1の次元を全て削除 

# アスペクト比固定で表示用画像をリサイズ
frame = scale_to_width(frame, 640)

# 検出されたすべての顔領域に対して１つずつ処理 
for detection in out:
    # conf値の取得 
    confidence = float(detection[2])

    # conf値が0.5より小さい場合はスキップ
    if confidence < 0.5:
        continue

    # バウンディングボックス座標を入力画像のスケールに変換 
    xmin = int(detection[3] * frame.shape[1])
    ymin = int(detection[4] * frame.shape[0])
    xmax = int(detection[5] * frame.shape[1])
    ymax = int(detection[6] * frame.shape[0])

    # 顔検出領域はカメラ範囲内に補正する。
    if xmin < 0:
        xmin = 0
    if ymin < 0:
        ymin = 0
    if xmax > frame.shape[1]:
        xmax = frame.shape[1]
    if ymax > frame.shape[0]:
        ymax = frame.shape[0]

    # 結果表示
    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color = (120, 90, 0), thickness = 2)

# 画像表示 
cv2.imshow('frame', frame)

# 何らかのキーが押されたら終了 
cv2.waitKey(0)
cv2.destroyAllWindows()
```



# 顔検出＋感情分類

上の2つを合体させ、顔検出した結果に感情分類を実行する。  
結果画像を表示するので、X環境での実行必須。  


### 準備

モデルデータは上の２つをそのまま使用。  

使用する画像をphoto.jpgとして保存しておく。  
または実行時にコマンドラインパラメータとしてファイル名を指定。  


### プログラム

試したソースはこちら。  

```python
# 顔検出 + 感情分析

# モジュール読み込み 
import sys
import cv2
import numpy as np
sys.path.append('/opt/intel/openvino/python/python3.7')
from openvino.inference_engine import IENetwork, IEPlugin

# =======================================================
# アスペクト比固定でリサイズする関数
def scale_to_width(img, width):
    scale = width / img.shape[1]
    return cv2.resize(img, dsize=None, fx=scale, fy=scale)

# =======================================================
# バウンティングボックスとラベルを表示する関数
def disp_Bounting_box(img, xmin, ymin, xmax, ymax, text, color) :
    # ラベル領域サイズ
    LABEL_W = len(text) * 12    # 等幅フォントじゃないので正確ではないけど、大体こんな感じ
    LABEL_H = 14                # こっちもトライアンドエラーで微調整
    # 画像サイズ
    IMAGE_W = img.shape[1]
    IMAGE_H = img.shape[0]
    
    # バウンディングボックス表示 
    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, thickness = 2)
    
    # ラベル表示位置(X)
    if (xmin + LABEL_W) > IMAGE_W :
        # 画面からはみ出さないように
        label_x = IMAGE_W - LABEL_W
    else :
        label_x = xmin
    
    # ラベル表示位置(Y)
    if (ymin - LABEL_H) < 0 :
        # 画面からはみ出さないように
        label_y = 0
    else :
        label_y = ymin - LABEL_H
    
    # 文字列背景描画(塗りつぶし)
    cv2.rectangle(img, (label_x, label_y), (label_x + LABEL_W, label_y + LABEL_H), color, thickness = -1)
    
    # 文字列描画(座標は文字の左下)
    cv2.putText(img, text, (label_x, label_y + LABEL_H), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.8, (255, 255, 255), 1)

# =======================================================
# 使用する画像ファイル名
image_filename = 'photo.jpg'

# パラメータが指定されていれば画像ファイルとして使用
args = sys.argv
if len(args) > 1 :
    image_filename = args[1]

# ターゲットデバイスの指定 
plugin = IEPlugin(device="MYRIAD")

# モデルの読み込み（顔検出） 
net_detect = IENetwork(model='FP16/face-detection-retail-0004.xml', weights='FP16/face-detection-retail-0004.bin')
exec_net_detect = plugin.load(network=net_detect)

# モデルの読み込み（感情分類） 
net_emotion = IENetwork(model='FP16/emotions-recognition-retail-0003.xml', weights='FP16/emotions-recognition-retail-0003.bin')
exec_net_emotion = plugin.load(network=net_emotion)

# 各感情の文字列をリスト化 
# 日本語表示にはPillow使うとかしないといけないので、英語で。
list_emotion    = ['neutral', 'happy', 'sad',    'surprise', 'anger']
list_emotion_jp = ['無表情',  '幸福',  '悲しみ', '驚き',     '怒り']

# 入力画像読み込み 
frame = cv2.imread(image_filename)

# 入力データフォーマットへ変換 
img = cv2.resize(frame, (300, 300)) # サイズ変更 
img = img.transpose((2, 0, 1))      # HWC > CHW 
img = np.expand_dims(img, axis=0)   # 次元合せ 

# 推論実行 
out = exec_net_detect.infer(inputs={'data': img})

# 出力から必要なデータのみ取り出し 
out = out['detection_out']
out = np.squeeze(out) #サイズ1の次元を全て削除 

# アスペクト比固定で表示用画像をリサイズ
frame = scale_to_width(frame, 640)

# 検出されたすべての顔領域に対して１つずつ処理 
for detection in out:
    # conf値の取得 
    confidence = float(detection[2])
    
    # conf値が0.5より小さい場合はスキップ
    if confidence < 0.5:
        continue
    
    # バウンディングボックス座標を入力画像のスケールに変換 
    xmin = int(detection[3] * frame.shape[1])
    ymin = int(detection[4] * frame.shape[0])
    xmax = int(detection[5] * frame.shape[1])
    ymax = int(detection[6] * frame.shape[0])
    
    # 顔検出領域はカメラ範囲内に補正する。特にminは補正しないとエラーになる
    if xmin < 0:
        xmin = 0
    if ymin < 0:
        ymin = 0
    if xmax > frame.shape[1]:
        xmax = frame.shape[1]
    if ymax > frame.shape[0]:
        ymax = frame.shape[0]
    
    # 顔領域のみ切り出し 
    frame_face = frame[ ymin:ymax, xmin:xmax ]
    
    # 入力データフォーマットへ変換 
    img_face = cv2.resize(frame_face, (64, 64))   # サイズ変更 
    img_face = img_face.transpose((2, 0, 1))    # HWC > CHW 
    img_face = np.expand_dims(img_face, axis=0) # 次元合せ 
    
    # 推論実行 
    out = exec_net_emotion.infer(inputs={'data': img_face})
    
    # 出力から必要なデータのみ取り出し 
    out = out['prob_emotion']
    out = np.squeeze(out) #不要な次元の削減 
    
    # 出力値が最大のインデックスを得る 
    index_max = np.argmax(out)
    
    # 結果表示
    # print(list_emotion[index_max])
    disp_Bounting_box(frame, xmin, ymin, xmax, ymax, list_emotion[index_max], (120, 90, 0))

# 画像表示 
cv2.imshow('frame', frame)

# 何らかのキーが押されたら終了 
cv2.waitKey(0)
cv2.destroyAllWindows()
```

# 考察  

基本パターンはこんな感じ。    

```python
# モジュールのロード
sys.path.append('/opt/intel/openvino/python/python3.7')
from openvino.inference_engine import IENetwork, IEPlugin

# 初期化
plugin = IEPlugin(device="MYRIAD")

# モデルの読み込み
net = IENetwork(model='xmlファイル', weights='binファイル')
exec_net = plugin.load(network=net)


# 推論実行 
# imgは入力データ、Numpy配列 ndarray型
out = exec_net.infer(inputs={'data': img})

# outが出力データ、Numpy配列 ndarray型

```

入出力データの並びは読み込んだモデルに依存する。  
入力データのサイズやRGB並び順などを合わせる。  
(モデルに入力する際にリサイズすれば、元の画像ファイルのサイズは任意で良い。)  
出力データはそのままでは「何のこっちゃ？」なので、きちんと整形してやる必要がある。   


ここに学習済みデータが色々登録されているらしい。  
<https://docs.openvinotoolkit.org/2019_R1/_docs_Pre_Trained_Models.html>  

TensorflowやCaffeで作成した学習済みデータを変換することもできるらしいが、やり方はこれから調査。  





# 参考情報
[openVINO toolkit リリースノート](https://software.intel.com/en-us/articles/OpenVINO-RelNotes)  
[openVINO toolkit リポジトリ](https://git.uni-paderborn.de/rnagle/dldt/tree/noctua_plugin_develop)  

