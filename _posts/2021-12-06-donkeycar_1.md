---
title: DonkeyCar simulatorã§å¼·åŒ–å­¦ç¿’(ãã®1)
date: 2021-12-06
tags: ["Tensorflow", "DeepLearning"]
excerpt: DonkeyCar simulatorã§å¼·åŒ–å­¦ç¿’ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã¿ã‚‹(DDQNç·¨)
---

# æ¦‚è¦
[DonkeyCar3ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§å¼·åŒ–å­¦ç¿’ã—ã¦ã¿ã‚‹](https://qiita.com/bathtimefish/items/a9b23681720527c0bd7e?fbclid=IwAR3sbaoBn09U7pFL4AKyEOXMi0wNXyAYi9jODUzO1muYr-N7q6hFG-hDfKs){:target="_blank"}ã®ãƒãƒã‚’ã—ã¦DonkeyCarã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸­ã«ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®ddqn.pyã‚’å®Ÿè¡Œã—ã¦ã¿ã‚‹ã€‚  
å‚è€ƒï¼š  
DQNã«ã¤ã„ã¦ã¯ã“ã“ãŒåˆ†ã‹ã‚Šã‚„ã™ã‹ã£ãŸã‹ãªã€‚  
[ã€æ·±å±¤å¼·åŒ–å­¦ç¿’,å…¥é–€ã€‘Deep Q Network(DQN)ã®è§£èª¬ã¨Pythonã§å®Ÿè£…ã€€ã€œå›³ã‚’ä½¿ã£ã¦èª¬æ˜ã€œ ](https://www.tcom242242.net/entry/ai-2/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/%E3%80%90%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%80%91deep_q_network_%E3%82%92tensorflow%E3%81%A7%E5%AE%9F%E8%A3%85/){:target="_blank"}  
[ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ã€‘Double Deep Q Network(DDQN)](https://www.tcom242242.net/entry/ai-2/%e5%bc%b7%e5%8c%96%e5%ad%a6%e7%bf%92/%e3%80%90%e6%b7%b1%e5%b1%a4%e5%bc%b7%e5%8c%96%e5%ad%a6%e7%bf%92%e3%80%91double-q-network/){:target="_blank"}  

# æº–å‚™

## pythonä»®æƒ³ç’°å¢ƒã®æº–å‚™
```bash
# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p /work2/donkey_sim
cd /work2/donkey_sim

# ä»®æƒ³ç’°å¢ƒæ§‹ç¯‰
pyenv virtualenv 3.8.11 donkey_sim
pyenv local donkey_sim 
pip install --upgrade pip setuptools wheel

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install opencv-python
pip install tensorflow

# ç¾æ™‚ç‚¹ã§ã®æœ€æ–°ãƒªãƒªãƒ¼ã‚¹ v21.7.24 ã‚’checkoutã—ã¦ãŠã
git clone https://github.com/tawnkramer/gym-donkeycar -b v21.07.24

# gym-donkeycarãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e gym-donkeycar
```
>[!NOTE]
> gym-donkeycar ã¯ -e (--editable) ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã‚‹ã®ã§ã€ç·¨é›†ã‚‚å¯èƒ½ã€‚ 
> git cloneã—ãŸå…ˆã‚’å‚ç…§ã—ã¦ã„ã‚‹ã®ã§ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã‚‚gym-donkeycarã‚’å‰Šé™¤ã—ã¡ã‚ƒãƒ€ãƒ¡ã€‚  
> 
> githubã‹ã‚‰ç›´æ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã€‚  
> ã“ã®å ´åˆã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯é€šå¸¸ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã€‚  
> ```bash
> pip install git+https://github.com/tawnkramer/gym-donkeycar@v21.07.24
> ```
> ã§ã‚‚ã€ä»¥ä¸‹ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ä½¿ã†ã®ã§ã€git cloneã‚‚ã—ãªã„ã¨ãƒ€ãƒ¡ã€‚  


## ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä»¥ä¸‹ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰å®Ÿè¡Œã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«åˆã‚ã›ã¦``DonkeySimXXXX.zip``(XXXXã¯ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€  
é©å½“ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å±•é–‹ã—ã¦ãŠãã¾ã™ã€‚  
(Linux/Macã®å ´åˆã¯å®Ÿè¡Œå±æ€§ä»˜ã‘ã‚‹ã®ã‚’å¿˜ã‚Œãšã«)  
<https://github.com/tawnkramer/gym-donkeycar/releases>

ãƒã‚·ãƒ³ã‚¹ãƒšãƒƒã‚¯ãŒãã‚Œã»ã©é«˜ããªã„å ´åˆã¯åˆ¥ãƒã‚·ãƒ³ã§å®Ÿè¡Œã—ã¦ãƒªãƒ¢ãƒ¼ãƒˆæ¥ç¶šã™ã‚‹ã®ãŒãŠã‚¹ã‚¹ãƒ¡ã€‚  
SSHæ¥ç¶šã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ãƒªãƒ¢ãƒ¼ãƒˆå¿…é ˆã€‚  

## patchã‚’ã‚ã¦ã‚‹

ä»¥ä¸‹ã®ãƒ‘ãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ãƒ‘ãƒƒãƒã‚’ã‚ã¦ã¾ã™ã€‚  
å†…å®¹ã¯ã€  
- ãªãœã‹``gym_donkeycar``ãŒimportã•ã‚Œã¦ãªã‹ã£ãŸ  
- tensorflow 1.13ä»¥é™2.0æœªæº€ç”¨ã®è¨­å®šã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‹ã‚‰ã‚¹ã‚­ãƒƒãƒ—ã§ãã‚‹ã‚ˆã†ã«ã—ãŸ  
- ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®ãƒªãƒ¢ãƒ¼ãƒˆå®Ÿè¡Œå¯¾å¿œ(hostã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ )  
-  æ¢ç´¢ç‡(Îµå€¤)ã®åˆæœŸå€¤è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¿½åŠ   
   æ¢ç´¢ç‡(Îµå€¤)ã«ã¤ã„ã¦ã¯[Îµ-greedyè¡Œå‹•é¸æŠ ](https://www.tcom242242.net/entry/ai-2/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/epsilon-greedy/){:target="_blank"}ã‚’å‚ç…§  

{% include filename.html filename="rl_sample.patch" %}
```patch
diff --git a/examples/reinforcement_learning/ddqn.py b/examples/reinforcement_learning/ddqn.py
index 87c74f0..5c32f49 100755
--- a/examples/reinforcement_learning/ddqn.py
+++ b/examples/reinforcement_learning/ddqn.py
@@ -21,6 +21,8 @@ from tensorflow.keras.layers import Activation, Conv2D, Dense, Flatten
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.optimizers import Adam
 
+import gym_donkeycar
+
 EPISODES = 10000
 img_rows, img_cols = 80, 80
 # Convert image into Black and white
@@ -121,6 +123,9 @@ class DQNAgent:
         if self.epsilon > self.epsilon_min:
             self.epsilon -= (self.initial_epsilon - self.epsilon_min) / self.explore
 
+    def set_epsilon(self, epsilon):
+        self.epsilon = epsilon
+
     def train_replay(self):
         if len(self.memory) < self.train_start:
             return
@@ -196,15 +201,17 @@ def run_ddqn(args):
     run a DDQN training session, or test it's result, with the donkey simulator
     """
 
-    # only needed if TF==1.13.1
-    config = tf.ConfigProto()
-    config.gpu_options.allow_growth = True
-    sess = tf.Session(config=config)
-    K.set_session(sess)
+    tf_ver = tf.__version__.split('.')
+    if (tf_ver[0] == 1 and tf_ver[1] >= 13) :
+        # only needed if TF==1.13.1
+        config = tf.ConfigProto()
+        config.gpu_options.allow_growth = True
+        sess = tf.Session(config=config)
+        K.set_session(sess)
 
     conf = {
         "exe_path": args.sim,
-        "host": "127.0.0.1",
+        "host": args.host,
         "port": args.port,
         "body_style": "donkey",
         "body_rgb": (128, 128, 128),
@@ -237,6 +244,9 @@ def run_ddqn(args):
     try:
         agent = DQNAgent(state_size, action_space, train=not args.test)
 
+        if args.epsilon > 0 :
+            agent.set_epsilon(args.epsilon)
+
         throttle = args.throttle  # Set throttle as constant value
 
         episodes = []
@@ -350,6 +360,7 @@ if __name__ == "__main__":
         default="manual",
         help="path to unity simulator. maybe be left at manual if you would like to start the sim on your own.",
     )
+    parser.add_argument("--host", type=str, default="127.0.0.1", help="simulator address")
     parser.add_argument("--model", type=str, default="rl_driver.h5", help="path to model")
     parser.add_argument("--test", action="store_true", help="agent uses learned model to navigate env")
     parser.add_argument("--port", type=int, default=9091, help="port to use for websockets")
@@ -357,6 +368,7 @@ if __name__ == "__main__":
     parser.add_argument(
         "--env_name", type=str, default="donkey-warehouse-v0", help="name of donkey sim environment", choices=env_list
     )
+    parser.add_argument("--epsilon", type=float, default=0.0, help="initial epsilon value")
 
     args = parser.parse_args()
```

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚  

```bash
cd gym-donkeycar/
patch -p1 < rl_sample.patch 
```


# ã¨ã‚Šã‚ãˆãšå­¦ç¿’
ã¨ã‚Šã‚ãˆãšå­¦ç¿’ã—ãªã„ã¨è©±ã«ãªã‚‰ãªã„ã®ã§å­¦ç¿’ã—ã¾ã™ã€‚  
å¼·åŒ–å­¦ç¿’ã¯æ•™å¸«ãƒ‡ãƒ¼ã‚¿ãŒè¦ã‚‰ãªã„ã®ã§ã€æº–å‚™ãŒãƒ©ã‚¯ãƒãƒ³...  ã§ã‚‚å­¦ç¿’ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹...  
DonkeyCar ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚·ãƒ³ã§å®Ÿè¡Œã™ã‚‹å ´åˆã€  
ãƒªãƒ¢ãƒ¼ãƒˆãƒã‚·ãƒ³ã§DonkeyCar ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’å®Ÿè¡Œã—ã¦ãŠãã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
``--sim``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«``remote``ã‚’ã€å®Ÿè¡Œãƒã‚·ãƒ³ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’``--host``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«æŒ‡å®šã—ã¾ã™ã€‚  
```bash
cd examples/reinforcement_learning
python ddqn.py --sim=remote --host=192.168.78.200
```
> [!NOTE]
> ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã¯  
> ``--sim``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«DonkeyCar ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿èµ·å‹•ã‚³ãƒãƒ³ãƒ‰ã‚’ãƒ•ãƒ«ãƒ‘ã‚¹ã§æŒ‡å®šã—ã¾ã™ã€‚  
> ã“ã®ã¨ãã€``--host``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ä¸è¦ã§ã™ã€‚  
> ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’ã‚ã‚‰ã‹ã˜ã‚èµ·å‹•ã—ã¦ãŠãå¿…è¦ã¯ãªãã€è‡ªå‹•çš„ã«èµ·å‹•ã•ã‚Œã¾ã™ã€‚  

ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ã«å­¦ç¿’çµæœãŒ ``rl_driver.h5``ã«ä¿å­˜ã•ã‚Œã‚‹ã®ã§ã€ä»»æ„ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§CTRL-Cã§ä¸­æ–­ã§ãã¾ã™ã€‚  
æ¬¡å›å­¦ç¿’ã‚’å†é–‹ã™ã‚‹å ´åˆã¯ã€ãƒ­ã‚°ã¨ã—ã¦è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹``epsilon: 0.XXXXXXX``ã®éƒ¨åˆ†ã®æœ€å¾Œã®å€¤ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚  
ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã¯Îµå€¤ã¯0.02ã‚’ä¸‹å›ã‚‹ã¨å›ºå®šã•ã‚Œã‚‹ã®ã§ã€ã‚ã‚‹ç¨‹åº¦å­¦ç¿’ãŒé€²ã‚“ã çŠ¶æ…‹ã§ã¯``0.02``ã ã¨æ€ã£ã¦ã‚‚å•é¡Œãªã„ã§ã—ã‚‡ã†ã€‚  

å­¦ç¿’ã‚’å†é–‹ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ä¸Šè¨˜ã‚³ãƒãƒ³ãƒ‰ã«``--epsilon``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚  
(``0.XXXXXXX``ã®éƒ¨åˆ†ã¯ä¸Šã§è¦šãˆã¦ãŠã„ãŸå€¤ã€‚ãƒ”ãƒƒã‚¿ãƒªåŒã˜ã§ãªãã¦å¤§ä½“ã§å¯)  
```bash
python ddqn.py --sim=remote --host=192.168.78.200 --epsilon=0.XXXXXXX
```

# å­¦ç¿’çµæœã§ãƒ†ã‚¹ãƒˆã—ã¦ã¿ã‚‹ã€‚  
å­¦ç¿’ã®ã¨ãã®ã‚³ãƒãƒ³ãƒ‰ã«``--test``ã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§ã™ã€‚  
``--epsilon``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æŒ‡å®šã—ã¾ã›ã‚“ã€‚  
```bash
python ddqn.py --sim=remote --host=192.168.78.200 --test
```
ã†ã¾ãå­¦ç¿’ãŒé€²ã‚“ã§ã„ã‚Œã°ã€ã‚³ãƒ¼ã‚¹ã‚¢ã‚¦ãƒˆã™ã‚‹ã“ã¨ãªãå‘¨å›ã—ã¦ãã‚Œã‚‹ãƒã‚ºã€‚  
å­¦ç¿’æ™‚ã¨åŒæ§˜ã€ã‚³ãƒ¼ã‚¹ã‚¢ã‚¦ãƒˆã™ã‚‹ã¨è‡ªå‹•çš„ã«ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®ã«æˆ»ã£ã¦å†ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¾ã™ã€‚  
é©å½“ã«CTRL-Cã§æ­¢ã‚ã¦ãã ã•ã„ã€‚  

# ã‚½ãƒ¼ã‚¹ã‚’èª­ã‚“ã§ã¿ã‚‹

ä»¥ä¸‹ã¯ã‚½ãƒ¼ã‚¹ã‚’èª­ã‚“ã æ™‚ã®ãƒ¡ãƒ¢ã§ã™ã€‚  
æ›¸ã„ã¦ã¿ãŸã‘ã©ã€è‡ªåˆ†ã§èª­ã‚“ã§ã‚‚ ãªã«ãŒä½•ã ã‹åˆ†ã‹ã‚‰ãªã„...ğŸ˜¢

## å†’é ­éƒ¨åˆ†
ã“ã®è¾ºã¯ãŠç´„æŸãªã®ã§ã€‚  

```python
"""
file: ddqn.py
author: Felix Yu
date: 2018-09-12
original: https://github.com/flyyufelix/donkey_rl/blob/master/donkey_rl/src/ddqn.py
"""
import argparse
import os
import random
import signal
import sys
import uuid
from collections import deque

import cv2
import gym
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Activation, Conv2D, Dense, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

```
## å†’é ­éƒ¨åˆ†ãã®2
``gym_donkey``ã‚’importã—ãªã„ã¨DonkeyCarã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨æ¥ç¶šã§ããªã„ã®ã§ã€‚  
ãªãœã‹ã‚ªãƒªã‚¸ãƒŠãƒ«ã§ã¯å…¥ã£ã¦ãªã‹ã£ãŸã€‚  
``if __name__ == "__main__":``ä»˜ã‘ã¨ã„ãŸæ–¹ãŒè‰¯ã„ã‹ã‚‚ã—ã‚Œã‚“ãŒã€ã¨ã‚Šã‚ãˆãšãã®ã¾ã¾importã—ã¦ãŠãã€‚  

```python
import gym_donkeycar

```

## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š  
æ„å‘³ã¯ä»¥ä¸‹ã®é€šã‚Šã€‚  

| å¤‰æ•° | æ„å‘³ |
|:--:|:--|
| EPISODES     | å­¦ç¿’å›æ•° |
| img_rows     | å…¥åŠ›ã«ä½¿ç”¨ã™ã‚‹ç”»åƒã‚µã‚¤ã‚º(Y) | 
| img_cols     | å…¥åŠ›ã«ä½¿ç”¨ã™ã‚‹ç”»åƒã‚µã‚¤ã‚º(X) | 
| img_channels | å…¥åŠ›ã«éå»ä½•ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã‹ | 

```python
EPISODES = 10000
img_rows, img_cols = 80, 80
# Convert image into Black and white
img_channels = 4  # We stack 4 frames
```

## å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¯ãƒ©ã‚¹

å¼·åŒ–å­¦ç¿’ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®šç¾©ã—ãŸã‚¯ãƒ©ã‚¹ã§ã™ã€‚
```python
class DQNAgent:
```
### ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
ã‚¯ãƒ©ã‚¹å¤‰æ•°  

| å¤‰æ•° | æ„å‘³ |
|:--:|:--|
|t              | å®Ÿè¡Œã‚«ã‚¦ãƒ³ã‚¿(ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºç”¨ã®ã¿ä½¿ç”¨)|
|max_Q          | Qå€¤ã®æœ€å¤§å€¤(ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºç”¨ã®ã¿ä½¿ç”¨) |
|train          | å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰/ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰(``--test``ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŒ‡å®š) |
|state_size     | ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›å±¤ã®ã‚µã‚¤ã‚ºã€‚ç¾çŠ¶æœªä½¿ç”¨  |
|action_space   | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®ç¾åœ¨ã®ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°/ã‚¹ãƒ­ãƒƒãƒˆãƒ«è¨­å®šå€¤å–å¾—ç”¨ |
|action_size    | æœªä½¿ç”¨ã€‚ãŸã¶ã‚“ã€ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°è§’ã‚’ä½•åˆ†å‰²ã™ã‚‹ã‹ã®å®šç¾©(15)ã«ã™ã¹ãã ã¨æ€ã† |
|discount_factor| å‰²å¼•ç‡(Î³å€¤) (å›ºå®šå€¤)|
|learning_rate  | å­¦ç¿’ç‡ (å›ºå®šå€¤)|
|epsilon        | ç¾åœ¨ã®æ¢ç´¢ç‡(Îµå€¤)|
|initial_epsilon| æ¢ç´¢ç‡ã®æœ€å¤§å€¤  æœ€å°ç‡ã¨å…±ã«æ¢ç´¢ç‡ã®å¤‰æ›´ç‡ã‚’è¨ˆç®—ã™ã‚‹(å›ºå®šå€¤)|
|epsilon_min    | æ¢ç´¢ç‡ã®æœ€å°å€¤ å­¦ç¿’æ™‚ã®æ¢ç´¢ç‡ã‚’ã“ã‚Œã‚ˆã‚Šå°ã•ãã—ãªã„(å›ºå®šå€¤)|
|explore        | æ¢ç´¢ç‡ã‚’æœ€å°å€¤ã«ã™ã‚‹ã¾ã§ã®å›æ•°(å›ºå®šå€¤)|
|batch_size     | ãƒãƒƒãƒã‚µã‚¤ã‚º (å›ºå®šå€¤)|
|train_start    | å­¦ç¿’é–‹å§‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°(æœ€åˆã¯å­¦ç¿’ã‚’è¡Œã‚ãªã„)(å›ºå®šå€¤)|
|memory         | Experience Buffer |
|model          | ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«| 
|target_model   | ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«(double DQNãªã®ã§)|

```python
    def __init__(self, state_size, action_space, train=True):
        self.t = 0
        self.max_Q = 0
        self.train = train

        # Get size of state and action
        self.state_size = state_size
        self.action_space = action_space
        self.action_size = action_space

        # These are hyper parameters for the DQN
        self.discount_factor = 0.99
        self.learning_rate = 1e-4
        if self.train:
            self.epsilon = 1.0
            self.initial_epsilon = 1.0
        else:
            self.epsilon = 1e-6
            self.initial_epsilon = 1e-6
        self.epsilon_min = 0.02
        self.batch_size = 64
        self.train_start = 100
        self.explore = 10000

        # Create replay memory using deque
        self.memory = deque(maxlen=10000)

        # Create main model and target model
        self.model = self.build_model()
        self.target_model = self.build_model()

        # Copy the model to target model
        # --> initialize the target model so that the parameters of model & target model to be same
        self.update_target_model()

```
### ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆ

ãã‚“ãªã«è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„ã¿ãŸã„ã€‚  
```python
    def build_model(self):
        model = Sequential()
        model.add(
            Conv2D(24, (5, 5), strides=(2, 2), padding="same", input_shape=(img_rows, img_cols, img_channels))
        )  # 80*80*4
        model.add(Activation("relu"))
        model.add(Conv2D(32, (5, 5), strides=(2, 2), padding="same"))
        model.add(Activation("relu"))
        model.add(Conv2D(64, (5, 5), strides=(2, 2), padding="same"))
        model.add(Activation("relu"))
        model.add(Conv2D(64, (3, 3), strides=(2, 2), padding="same"))
        model.add(Activation("relu"))
        model.add(Conv2D(64, (3, 3), strides=(1, 1), padding="same"))
        model.add(Activation("relu"))
        model.add(Flatten())
        model.add(Dense(512))
        model.add(Activation("relu"))

        # 15 categorical bins for Steering angles
        model.add(Dense(15, activation="linear"))

        adam = Adam(lr=self.learning_rate)
        model.compile(loss="mse", optimizer=adam)

        return model

```
### RGBâ†’ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›å‡¦ç†
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®å‡ºåŠ›ã¯RGBç”»åƒã€ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¯ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒãªã®ã§ã€ãã®å¤‰æ›ã‚’è¡Œã†ãŸã‚ã®é–¢æ•°ã€‚  
``cv2.dst = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)`` ã§è‰¯ã„æ°—ã‚‚ã™ã‚‹ãŒ...  

```python
    def rgb2gray(self, rgb):
        """
        take a numpy rgb image return a new single channel image converted to greyscale
        """
        return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])

```

### å…¥åŠ›ç”»åƒå‰å‡¦ç†
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®å‡ºåŠ›ç”»åƒã‚’ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹å‡¦ç†ã€‚  
RGBã‹ã‚‰ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›ã—ã€ãƒªã‚µã‚¤ã‚ºã‚’è¡Œã†ã€‚  

```python
    def process_image(self, obs):
        obs = self.rgb2gray(obs)
        obs = cv2.resize(obs, (img_rows, img_cols))
        return obs

```

### ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°

ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹  

```python
    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

```

### ç¾åœ¨ã®ç’°å¢ƒã§ã®æ¬¡ã®è¡Œå‹•ã‚’å–å¾—ã™ã‚‹  

ä¹±æ•°ã‚’ç™ºç”Ÿã—ã€Îµå€¤ä»¥ä¸‹ã ã£ãŸã‚‰ç’°å¢ƒãŒç”Ÿæˆã—ãŸãƒ©ãƒ³ãƒ€ãƒ å€¤(``self.action_space.sample()[0]``)ã‚’è¿”ã™ã€‚  
ãã‚Œä»¥å¤–ã¯ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã—ãŸçµæœã‚’è¿”ã™ã€‚  
ãã®éš›ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›çµæœãã®ã¾ã¾ã§ã¯ãªãã€ã©ã®ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°ä½ç½®ã«å½“ãŸã‚‹ã‹ã®é‡å­åŒ–ã‚’è¡Œã£ã¦è¿”ã™ã€‚  
(å¾—ã‚‰ã‚Œã‚‹ã®ã¯ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°æƒ…å ±ã ã‘ã§ã€ã‚¹ãƒ­ãƒƒãƒˆãƒ«æƒ…å ±ã¯å›ºå®šå€¤)  

```python
    # Get action from model using epsilon-greedy policy
    def get_action(self, s_t):
        if np.random.rand() <= self.epsilon:
            return self.action_space.sample()[0]
        else:
            # print("Return Max Q Prediction")
            q_value = self.model.predict(s_t)

            # Convert q array to steering value
            return linear_unbin(q_value[0])
```

### çŠ¶æ…‹ç­‰ã®ä¿å­˜
ç¾åœ¨ã®çŠ¶æ…‹(state)ã€è¡Œå‹•(action)ã€å ±é…¬(reward)ã€è¡Œå‹•å¾Œã®çŠ¶æ…‹(next_state)ã€
çµ‚äº†ãƒ•ãƒ©ã‚°(done)ã‚’Experience Bufferã«ä¿å­˜ã™ã‚‹ã€‚  
(Experience Buffer ã¯ Experience Replayã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ãŠãã¨ã“ã‚)  
``memory`` ã¯ ``collections.dque()``ã§ä½œæˆã—ã¦ã„ã‚‹ã®ã§ã€æŒ‡å®šã‚µã‚¤ã‚ºã‚’è¶…ãˆãŸã¨ãã¯å¤ã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é †ã«å‰Šé™¤ã•ã‚Œã‚‹ã€‚  

```python
    def replay_memory(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

```

### Îµå€¤ã®æ›´æ–°

ç¾åœ¨ã®Îµå€¤ãŒæœ€å°å€¤ã‚ˆã‚Šå¤§ãã‹ã£ãŸã‚‰ä¸€å®šæ¯”ç‡ã§å°ã•ãã—ã¦ã„ãã€‚  
æœ€å°å€¤ä»¥ä¸‹ã«ãªã£ã¦ã„ãŸã‚‰ãã®ã¾ã¾ã€‚  

```python
    def update_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon -= (self.initial_epsilon - self.epsilon_min) / self.explore

```

### Îµå€¤ã®åˆæœŸè¨­å®š

``--epsilon``ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ ã—ãŸã®ã§ã€æŒ‡å®šå€¤ã§Îµå€¤ã‚’å¤‰æ›´ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ ã€‚  

```python
    def set_epsilon(self, epsilon):
        self.epsilon = epsilon
```

### å­¦ç¿’

Experience Bufferã‹ã‚‰ä»»æ„ã®çµŒé¨“ã‚’å–ã‚Šå‡ºã—ã€Q Networkã‚’ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’(Experience Replay)

è¨˜æ†¶ã—ãŸãƒ‡ãƒ¼ã‚¿æ•°ãŒ``self.train_start``ã«é”ã™ã‚‹ã¾ã§ã¯ä½•ã‚‚ã—ãªã„ã€‚  
ãƒãƒƒãƒå­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’Experience Bufferã‹ã‚‰å–ã‚Šå‡ºã—ã€  
ãã‚Œãã‚Œã®é…åˆ—ã«ãƒãƒ©ã™(``state_t``,``action_t``, ``reward_t``, ``state_t1``, ``terminal``)ã€‚  
``state_t``ã¨``state_t1``ã¯``np.concatenate()``ã§ndarrayã«ã¾ã¨ã‚ã¦ãŠãã€‚  
11è¡Œç›®ã®``self.model.predict(state_t)``ã¯``self.max_Q``ã®å–å¾—ã«ã—ã‹ä½¿ç”¨ã•ã‚Œã¦ãŠã‚‰ãšã€   

``self.max_Q``ã¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºã«ã—ã‹ä½¿ç”¨ã•ã‚Œã¦ãªãã€ç„¡é§„ãªè¨ˆç®—ãªã®ã§ã€å‰Šé™¤ã™ã‚‹ã®ãŒè‰¯ã„ã¨æ€ã‚ã‚Œã‚‹(ç„¡é§„ãªè¨ˆç®—ãªã®ã§)ã€‚  
ãã®å ´åˆã€``targets``ã®åˆæœŸåŒ–ã¯``targets = np.zeros((batch_size, 15))``ã§è¡Œã†ã€‚  
(``self.max_Q``ã‚’å‚ç…§ã—ã¦ã„ã‚‹ã¨ã“ã‚ã‚‚å‰Šé™¤ã€‚ã‚ã‚‹ã„ã¯``get_action()``ã§æˆ»ã‚Šå€¤ã¨ã—ã¦è¿”ã™ã®ã‚‚æ‰‹ã‹ã€‚)   

``state_t1``ã‚’å…¥åŠ›ã¨ã—ã¦ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å¾—ã‚‰ã‚ŒãŸå‡ºåŠ›ã‹ã‚‰å‡ºåŠ›æœŸå¾…å€¤ã‚’å–å¾—ã—ã€  
å­¦ç¿’ã‚’è¡Œã†``self.model.train_on_batch(state_t, targets)``ã€‚  
ã“ã®è¾ºã¯
[ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ã€‘Double Deep Q Network(DDQN)](https://www.tcom242242.net/entry/ai-2/%e5%bc%b7%e5%8c%96%e5%ad%a6%e7%bf%92/%e3%80%90%e6%b7%b1%e5%b1%a4%e5%bc%b7%e5%8c%96%e5%ad%a6%e7%bf%92%e3%80%91double-q-network/){:target="_blank"}
ã®ã‚½ãƒ¼ã‚¹ã¨ã‹ã‚’è¦‹ã‚‹ã¨åˆ†ã‹ã£ãŸã‚ˆã†ãªåˆ†ã‹ã‚‰ãªã„ã‚ˆã†ãªæ°—ã«ãªã‚Œã‚‹ã‹ã‚‚...  

```python
    def train_replay(self):
        if len(self.memory) < self.train_start:
            return

        batch_size = min(self.batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)

        state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)
        state_t = np.concatenate(state_t)
        state_t1 = np.concatenate(state_t1)
        targets = self.model.predict(state_t)
        self.max_Q = np.max(targets[0])
        target_val = self.model.predict(state_t1)
        target_val_ = self.target_model.predict(state_t1)
        for i in range(batch_size):
            if terminal[i]:
                targets[i][action_t[i]] = reward_t[i]
            else:
                a = np.argmax(target_val[i])
                targets[i][action_t[i]] = reward_t[i] + self.discount_factor * (target_val_[i][a])

        self.model.train_on_batch(state_t, targets)

```

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å…ˆã¯ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã€‚  
ã“ã®ã‚ã¨ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¸ã‚³ãƒ”ãƒ¼ã™ã‚‹ã®ã§ã€ã“ã“ã§ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã¯è§¦ã‚‰ãªã„ã€‚  

```python
    def load_model(self, name):
        self.model.load_weights(name)

```

### ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜

ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚  
```python
    # Save the model which is under training
    def save_model(self, name):
        self.model.save_weights(name)
```

## ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°è§’â†’ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›å½¢å¼å¤‰æ›

ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°è§’(-1ï½1)ã‚’ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›å½¢å¼(è¦ç´ æ•°15ã®é…åˆ—ã®ã©ã‚Œã‹1ã¤ã«1ãŒå…¥ã‚‹)ã«å¤‰æ›ã™ã‚‹ã€‚  

```python
def linear_bin(a):
    """
    Convert a value to a categorical array.

    Parameters
    ----------
    a : int or float
        A value between -1 and 1

    Returns
    -------
    list of int
        A list of length 15 with one item set to 1, which represents the linear value, and all other items set to 0.
    """
    a = a + 1
    b = round(a / (2 / 14))
    arr = np.zeros(15)
    arr[int(b)] = 1
    return arr
```


## ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›å½¢å¼â†’ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°è§’å¤‰æ›

ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®ã†ã¡ã€æœ€å¤§å€¤ã‚’æŒã¤indexã«ç›¸å½“ã™ã‚‹ã‚¹ãƒ†ã‚¢ãƒªãƒ³ã‚°è§’ã‚’å–å¾—ã™ã‚‹ã€‚  

```python
def linear_unbin(arr):
    """
    Convert a categorical array to value.

    See Also
    --------
    linear_bin
    """
    if not len(arr) == 15:
        raise ValueError("Illegal array length, must be 15")
    b = np.argmax(arr)
    a = b * (2 / 14) - 1
    return a
```


## ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒãƒ³

```python
def run_ddqn(args):
    """
    run a DDQN training session, or test it's result, with the donkey simulator
    """
```

### Tensorflow 1.13.1ã§ã®ãŠã¾ã˜ãªã„

Tensorflow 2 ã‚’ä½¿ç”¨ã—ãŸã‹ã£ãŸã®ã§ã€å‡¦ç†ä¸è¦ã€‚  
ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚Œã°è‰¯ã„ã®ã ã‘ã‚Œã©ã€ãªã‚“ã¨ãªããƒãƒ¼ã‚¸ãƒ§ãƒ³ã§åˆ†ã‘ã¦ã¿ãŸã€‚  
1.14ä»¥é™ã§ã¯è¦ã‚‹ã®ã‹ãªï¼Ÿè¦ã‚‹ã¨æ€ã£ã¦æ›¸ã„ã¦ã¿ãŸã€‚  

```python
    tf_ver = tf.__version__.split('.')
    if (tf_ver[0] == 1 and tf_ver[1] >= 13) :
        # only needed if TF==1.13.1
        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.Session(config=config)
        K.set_session(sess)
```

### ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ç’°å¢ƒã®æ§‹ç¯‰

``body_style`` ã«ã¯ ``donkey``ã€ ``bare``ã€``car01``ã€``cybertruck``ã€``f1``ãŒä½¿ç”¨ã§ãã‚‹ã‚‰ã—ã„ã€‚  
``body_rgb`` ã§ è‰²ã‚’æŒ‡å®šã§ãã‚‹ã‚‰ã—ã„ã€‚  
``car_name`` ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã«è¡¨ç¤ºã•ã‚Œã‚‹åå‰ã‚’æŒ‡å®šã€‚è¤‡æ•°ã®è»Šã‚’èµ°ã‚‰ã›ã‚‹ã¨ãã«è¦‹åˆ†ã‘ã‚‰ã‚Œã‚‹ã¿ãŸã„ã€‚  
``font_size``ã§åå‰ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’æŒ‡å®šã€‚  

```python
    conf = {
        "exe_path": args.sim,
        "host": args.host,
        "port": args.port,
        "body_style": "donkey",
        "body_rgb": (128, 128, 128),
        "car_name": "me",
        "font_size": 100,
        "racer_name": "DDQN",
        "country": "USA",
        "bio": "Learning to drive w DDQN RL",
        "guid": str(uuid.uuid4()),
        "max_cte": 10,
    }

    # Construct gym environment. Starts the simulator if path is given.
    env = gym.make(args.env_name, conf=conf)
```

### ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã®ãƒ•ãƒƒã‚¯ãƒ«ãƒ¼ãƒãƒ³ã®å®šç¾©ã¨ç™»éŒ²

ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã®çµ‚äº†å‡¦ç†ã‚’è¡Œã†ã‚ˆã†ã«ãƒ•ãƒƒã‚¯ãƒ«ãƒ¼ãƒãƒ³ã‚’ç™»éŒ²ã™ã‚‹ã€‚  

```python
    # not working on windows...
    def signal_handler(signal, frame):
        print("catching ctrl+c")
        env.unwrapped.close()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGABRT, signal_handler)

```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”¨å¤‰æ•°ã®å®šç¾©

```python
    # Get size of state and action from environment
    state_size = (img_rows, img_cols, img_channels)
    action_space = env.action_space  # Steering and Throttle
```


### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç”Ÿæˆ

```python
    try:
        agent = DQNAgent(state_size, action_space, train=not args.test)
```

### Îµå€¤ã®è¨­å®š

ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Îµå€¤ãŒæŒ‡å®šã•ã‚Œã¦ã„ãŸã‚‰è¨­å®šã™ã‚‹ã€‚  

```python
        if args.epsilon > 0 :
            agent.set_epsilon(args.epsilon)
```

### ã‚¹ãƒ­ãƒƒãƒˆãƒ«å€¤ã®è¨­å®š

ã‚¹ãƒ­ãƒƒãƒˆãƒ«ã®è¨­å®šã¯å›ºå®šå€¤(ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®š)   

```python
        throttle = args.throttle  # Set throttle as constant value

        episodes = []
```

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€ã€‚  

```python
        if os.path.exists(args.model):
            print("load the saved model")
            agent.load_model(args.model)

```

### å­¦ç¿’ãƒ«ãƒ¼ãƒ—
```python
        for e in range(EPISODES):

            print("Episode: ", e)
```

#### ã‚¹ã‚¿ãƒ¼ãƒˆä½ç½®ã¸ç§»å‹•

``obs`` â† ã‚¹ã‚¿ãƒ¼ãƒˆæ™‚ã®ã‚«ãƒ¡ãƒ©ç”»åƒ  
``x_t`` â† ``obs`` ã‚’ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›å½¢å¼ã«åˆã‚ã›ã¦å¤‰æ›(ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«åŒ–ï¼†ãƒªã‚µã‚¤ã‚º)   
``s_t`` â† ``x_t``ã‚’4æšåˆ†ã‚³ãƒ”ãƒ¼(å…¥åŠ›ç”»åƒã¯éå»4æšåˆ†ã‚’ä½¿ç”¨ã™ã‚‹ã®ã§)(ã¡ã‚ƒã‚“ã¨``img_channels``å‚ç…§ã—ã¦æ¬²ã—ã„ã‘ã©)   
```python
            done = False
            obs = env.reset()

            episode_len = 0

            x_t = agent.process_image(obs)

            s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)
            # In Keras, need to reshape
            s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  # 1*80*80*4
```

#### ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ«ãƒ¼ãƒ—

çµ‚äº†ãƒ•ãƒ©ã‚°ãŒã‚»ãƒƒãƒˆã•ã‚Œã‚‹ã¾ã§ãƒ«ãƒ¼ãƒ—   
```python
            while not done:
```

##### ç¾åœ¨ã®çŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã‚’äºˆæ¸¬ã—ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§å®Ÿè¡Œ

``steering`` â† äºˆæ¸¬çµæœ  
``env.step()``ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ  
``x_t1`` â†ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œå¾Œã®ã‚«ãƒ¡ãƒ©ç”»åƒ  
``s_t1`` â† ç¾åœ¨ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ä¸€ç•ªå¤ã„ã‚‚ã®ã‚’å‰Šé™¤ã—ã€ä»Šå›ã®ç”»åƒã‚’è¿½åŠ   

```python
                # Get action for the current state and go one step in environment
                steering = agent.get_action(s_t)
                action = [steering, throttle]
                next_obs, reward, done, info = env.step(action)

                x_t1 = agent.process_image(next_obs)

                x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1)  # 1x80x80x1
                s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)  # 1x80x80x4
```

##### Experience Bufferã«ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜

Îµå€¤ã®æ›´æ–°ã‚‚

```python
                # Save the sample <s, a, r, s'> to the replay memory
                agent.replay_memory(s_t, np.argmax(linear_bin(steering)), reward, s_t1, done)
                agent.update_epsilon()
```

##### å­¦ç¿’å®Ÿè¡Œ

```python
                if agent.train:
                    agent.train_replay()
```

##### ãƒ«ãƒ¼ãƒ—æ›´æ–°å‡¦ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º

```python
                s_t = s_t1
                agent.t = agent.t + 1
                episode_len = episode_len + 1
                if agent.t % 30 == 0:
                    print(
                        "EPISODE",
                        e,
                        "TIMESTEP",
                        agent.t,
                        "/ ACTION",
                        action,
                        "/ REWARD",
                        reward,
                        "/ EPISODE LENGTH",
                        episode_len,
                        "/ Q_MAX ",
                        agent.max_Q,
                    )
```
##### ãƒ«ãƒ¼ãƒ—æ›´æ–°å‡¦ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º

``agent.update_target_model()``ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
``episodes.append(e)``ã¯ãƒ‡ãƒãƒƒã‚°ç”¨ï¼Ÿ


```python
                if done:

                    # Every episode update the target model to be same with model
                    agent.update_target_model()

                    episodes.append(e)

                    # Save model for each episode
                    if agent.train:
                        agent.save_model(args.model)

                    print(
                        "episode:",
                        e,
                        "  memory length:",
                        len(agent.memory),
                        "  epsilon:",
                        agent.epsilon,
                        " episode length:",
                        episode_len,
                    )
```
#### ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ«ãƒ¼ãƒ—ã¨å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®çµ‚ã‚ã‚Š
ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿ä¾‹å¤–ã¨çµ‚äº†å‡¦ç†
```python
    except KeyboardInterrupt:
        print("stopping run...")
    finally:
        env.unwrapped.close()
```


## ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³è§£æå‡¦ç†ã¾ã‚ã‚Š

ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³è§£æå‡¦ç†ã¨ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒãƒ³ã¸ã®ã‚¸ãƒ£ãƒ³ãƒ—  

```python
if __name__ == "__main__":

    # Initialize the donkey environment
    # where env_name one of:
    env_list = [
        "donkey-warehouse-v0",
        "donkey-generated-roads-v0",
        "donkey-avc-sparkfun-v0",
        "donkey-generated-track-v0",
        "donkey-roboracingleague-track-v0",
        "donkey-waveshare-v0",
        "donkey-minimonaco-track-v0",
        "donkey-warren-track-v0",
        "donkey-thunderhill-track-v0",
        "donkey-circuit-launch-track-v0",
    ]

    parser = argparse.ArgumentParser(description="ddqn")
    parser.add_argument(
        "--sim",
        type=str,
        default="manual",
        help="path to unity simulator. maybe be left at manual if you would like to start the sim on your own.",
    )
    parser.add_argument("--host", type=str, default="127.0.0.1", help="simulator address")
    parser.add_argument("--model", type=str, default="rl_driver.h5", help="path to model")
    parser.add_argument("--test", action="store_true", help="agent uses learned model to navigate env")
    parser.add_argument("--port", type=int, default=9091, help="port to use for websockets")
    parser.add_argument("--throttle", type=float, default=0.3, help="constant throttle for driving")
    parser.add_argument(
        "--env_name", type=str, default="donkey-warehouse-v0", help="name of donkey sim environment", choices=env_list
    )
    parser.add_argument("--epsilon", type=float, default=0.0, help="initial epsilon value")

    args = parser.parse_args()

    run_ddqn(args)
```


## ã§ã‚‚ã£ã¦ã€ã“ã‚“ãªæ”¹é€ ã‚’ã™ã‚‹ã¨ã¡ã‚‡ã³ã£ã¨è¨ˆç®—é‡ãŒæ¸›ã‚‹
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã«è¡¨ç¤ºã•ã‚Œã‚‹è»Šã‚’å¤‰æ›´ã—ã¦ã‚‹ã®ã¯ã”æ„›æ•¬ğŸ˜…
```diff
--- ddqn.py.old	2021-12-02 06:25:02.149997073 +0900
+++ ddqn.py	2021-12-03 07:14:49.346378878 +0900
@@ -98,9 +98,10 @@
         return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])
 
     def process_image(self, obs):
-        obs = self.rgb2gray(obs)
-        obs = cv2.resize(obs, (img_rows, img_cols))
-        return obs
+        # obs1 = self.rgb2gray(obs)
+        obs1 = cv2.dst = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
+        obs2 = cv2.resize(obs1, (img_rows, img_cols))
+        return obs2
 
     def update_target_model(self):
         self.target_model.set_weights(self.model.get_weights())
@@ -108,13 +109,17 @@
     # Get action from model using epsilon-greedy policy
     def get_action(self, s_t):
         if np.random.rand() <= self.epsilon:
-            return self.action_space.sample()[0]
+            return self.action_space.sample()[0], 0
         else:
             # print("Return Max Q Prediction")
             q_value = self.model.predict(s_t)
 
+            max_q = np.amax(q_value[0])
+            if self.max_Q < max_q :
+                self.max_Q = max_q
+
             # Convert q array to steering value
-            return linear_unbin(q_value[0])
+            return linear_unbin(q_value[0]), max_q
 
     def replay_memory(self, state, action, reward, next_state, done):
         self.memory.append((state, action, reward, next_state, done))
@@ -136,16 +141,16 @@
         state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)
         state_t = np.concatenate(state_t)
         state_t1 = np.concatenate(state_t1)
-        targets = self.model.predict(state_t)
-        self.max_Q = np.max(targets[0])
-        target_val = self.model.predict(state_t1)
-        target_val_ = self.target_model.predict(state_t1)
+
+        targets = np.zeros((batch_size, 15))
+        q_val = self.model.predict(state_t1)
+        target_q_val = self.target_model.predict(state_t1)
         for i in range(batch_size):
             if terminal[i]:
                 targets[i][action_t[i]] = reward_t[i]
             else:
-                a = np.argmax(target_val[i])
-                targets[i][action_t[i]] = reward_t[i] + self.discount_factor * (target_val_[i][a])
+                a = np.argmax(q_val[i])
+                targets[i][action_t[i]] = reward_t[i] + self.discount_factor * (target_q_val[i][a])
 
         self.model.train_on_batch(state_t, targets)
 
@@ -213,8 +218,8 @@
         "exe_path": args.sim,
         "host": args.host,
         "port": args.port,
-        "body_style": "donkey",
-        "body_rgb": (128, 128, 128),
+        "body_style": "f1",
+        "body_rgb": (255, 128, 128),
         "car_name": "me",
         "font_size": 100,
         "racer_name": "DDQN",
@@ -273,7 +278,7 @@
             while not done:
 
                 # Get action for the current state and go one step in environment
-                steering = agent.get_action(s_t)
+                steering, max_Q = agent.get_action(s_t)
                 action = [steering, throttle]
                 next_obs, reward, done, info = env.step(action)
 
@@ -305,7 +310,7 @@
                         "/ EPISODE LENGTH",
                         episode_len,
                         "/ Q_MAX ",
-                        agent.max_Q,
+                        max_Q,
                     )
 
                 if done:
```